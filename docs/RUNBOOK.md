## RUNBOOK: Weekly snapshot refresh & pipeline run

This project supports a weekly pipeline that can run using a pinned CACHE_DATE in CI.

- Monthly maintenance: the maintainer should refresh the cached snapshot (under specs/002-use-nfl-game/data/raw) at the start of each month and update `.github/workflows/weekly-pipeline.yml` `CACHE_DATE` default input.
- To refresh manually locally: run `scripts/ingest_nflreadpy.py --live --out-dir specs/002-use-nfl-game/data/raw/<YYYYMMDD>` then run `scripts/build_features.py`.
- After updating the snapshot, commit the new raw snapshot manifest and bump the `CACHE_DATE` value in the workflow.

Keep all runs reproducible by recording the run manifest files generated by `run_manifest.py`.

---

## Metrics YAML schema (required)

All training runs that produce metrics must write a YAML containing the following keys exactly:

- season: integer (e.g. 2018)
- week: integer (e.g. 1)
- model: string (e.g. 'logreg' or 'booster')
- model_hash: string (hex SHA256-like identifier)
- accuracy: float
- brier: float
- logloss: float
- ece: float
- acceptance_pass: boolean

Example metrics YAML:

```yaml
season: 2018
week: 1
model: logreg
model_hash: 9357c91026dedf84a57354df59b892c86d14b37d238e0396430b1bddb6d72e85
accuracy: 0.61
brier: 0.20
logloss: 0.60
ece: 0.03
acceptance_pass: true
```

## DuckDB: weekly_metric_trends view

The project creates a `weekly_metrics` table and a `weekly_metric_trends` view that computes 4-week rolling means for key metrics.

SQL to create the trends view (example):

```sql
CREATE TABLE IF NOT EXISTS weekly_metrics (
  season INTEGER,
  week INTEGER,
  model_hash VARCHAR,
  dataset VARCHAR,
  accuracy DOUBLE,
  brier DOUBLE,
  logloss DOUBLE,
  ece DOUBLE,
  created_at TIMESTAMP
);

CREATE OR REPLACE VIEW weekly_metric_trends AS
SELECT
  season,
  week,
  accuracy,
  brier,
  logloss,
  ece,
  avg(accuracy) OVER (ORDER BY season, week ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS accuracy_4wk_mean,
  avg(brier) OVER (ORDER BY season, week ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS brier_4wk_mean,
  avg(logloss) OVER (ORDER BY season, week ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS logloss_4wk_mean,
  avg(ece) OVER (ORDER BY season, week ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS ece_4wk_mean
FROM weekly_metrics
ORDER BY season, week;
```

Example queries:

1) Per-week metrics for 2018:

```sql
SELECT season, week, accuracy, brier, logloss, ece FROM weekly_metrics WHERE season=2018 ORDER BY week;
```

2) Trends with 4-week rolling means:

```sql
SELECT * FROM weekly_metric_trends WHERE season=2018 ORDER BY week;
```

MVP Runbook
============

This project ships an MVP pipeline that uses real NFL data via `nflreadpy`, writes Parquet-only snapshots, builds partitioned features, trains deterministic models (baseline + booster), runs gating checks (license, leakage, determinism, prediction contract, acceptance), and registers features in DuckDB for analytics.

Quick flow (developer)
- Live ingest (manual):
  - python3 specs/002-use-nfl-game/scripts/ingest_nflreadpy.py --out specs/002-use-nfl-game/data/raw --start-season 2018 --end-season 2023 --live
  - This writes: `specs/002-use-nfl-game/data/raw/<YYYYMMDD>/team_week_raw.parquet` and a manifest under `.../manifests/` containing `data_manifest_id` and `source_urls`.
- Build features (Parquet-only):
  - python3 specs/002-use-nfl-game/scripts/build_features.py --raw specs/002-use-nfl-game/data/raw/<YYYYMMDD> --out specs/002-use-nfl-game/data/features --min-games 2
  - This writes partitioned Parquet: `specs/002-use-nfl-game/data/features/season=YYYY/features.parquet` and `features.metadata.json` per partition with proxy flags and row counts.
- Train & validate (deterministic gates):
  - python3 specs/002-use-nfl-game/scripts/train_and_validate.py
  - Runs license check, leakage smoke, trains baseline twice with same seeds and asserts identical `model_hash`, validates prediction CSV contract, and writes metrics YAML containing `acceptance_pass`.
- Register DuckDB (analytics):
  - python3 specs/002-use-nfl-game/scripts/register_duckdb.py --features-dir specs/002-use-nfl-game/data/features --out-db specs/002-use-nfl-game/db/features.duckdb
  - DuckDB registers a view over the partitioned Parquet files for easy queries.

CI snapshot policy
- CI must not perform live network fetches. CI uses a cached snapshot under `specs/002-use-nfl-game/data/raw_cache/<YYYYMMDD>/`.
- The workflow `ci-mvp.yml` exposes `CACHE_DATE` as an input (default `20250924`). Change the input or patch the workflow to use another cached date.

Notes
- Parquet-only I/O: the pipeline uses Parquet exclusively; no CSV fallbacks remain.
- Proxies: when `epa` or `success_rate` are missing, proxies are used and flagged in `features.metadata.json` under each partition.
- Licenses: packages are SPDX-normalized; no UNKNOWN licenses in the prepared run.
